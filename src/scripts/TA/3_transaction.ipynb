{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "import threading\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'persistent': {}, 'transient': {'cluster': {'routing': {'allocation': {'disk': {'watermark': {'low': '92%', 'flood_stage': '97%', 'high': '95%'}}}}}}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch(\n",
    "        \"http://localhost:9200\",\n",
    ")\n",
    "if es.ping():\n",
    "    print(\"Connected to Elasticsearch\")\n",
    "else:\n",
    "    print(\"Could not connect to Elasticsearch\")\n",
    "\n",
    "es.cluster.put_settings(\n",
    "    transient={\n",
    "        \"cluster.routing.allocation.disk.watermark.low\": \"92%\",\n",
    "        \"cluster.routing.allocation.disk.watermark.high\": \"95%\",\n",
    "        \"cluster.routing.allocation.disk.watermark.flood_stage\": \"97%\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (575, 2)\n",
      "   Patient ID                                  Clinician's Notes\n",
      "0           1  L4-5: degenerative annular disc bulge is noted...\n",
      "1           2  No evidence of disc herniation.\\nNo significan...\n",
      "2           3  LSS MRI\\nFeatures of muscle spasm.\\nsmall cent...\n",
      "3           4  Feature of muscle spasm.\\nDiffuse disc bulges ...\n",
      "4           5  LSS MRI :\\nFeature of muscle spasm.\\nDiffuse d...\n"
     ]
    }
   ],
   "source": [
    "excel_file = \"../../data/text/Radiologists Report.xlsx\"\n",
    "TEXT_INDEX = \"radiology_reports\"\n",
    "df = pd.read_excel(excel_file)\n",
    "df_clean = df.dropna(subset=['Patient ID', \"Clinician's Notes\"])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'radiology_reports'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define index mapping, with 1 shard and 1 replica\n",
    "index_name = TEXT_INDEX\n",
    "index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"patient_id\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"clinicians_notes\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    },\n",
    "                    \"english\": {\n",
    "                        \"type\": \"text\",\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 2,\n",
    "        \"number_of_replicas\": 1,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Delete index if it exists\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name, timeout='2m')\n",
    "\n",
    "# # Observe indices\n",
    "# indices = es.cat.indices(format=\"json\")  # list of dicts\n",
    "# for row in indices:\n",
    "#     print(row[\"health\"], row[\"status\"], row[\"index\"], row[\"docs.count\"], row[\"store.size\"])\n",
    "\n",
    "\n",
    "# Create index\n",
    "es.indices.create(index=index_name, body=index_mapping, timeout='2m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"index\": \"radiology_reports\",\n",
      "    \"shard\": \"0\",\n",
      "    \"prirep\": \"r\",\n",
      "    \"node\": \"es_data\"\n",
      "  },\n",
      "  {\n",
      "    \"index\": \"radiology_reports\",\n",
      "    \"shard\": \"0\",\n",
      "    \"prirep\": \"p\",\n",
      "    \"node\": \"es_master\"\n",
      "  },\n",
      "  {\n",
      "    \"index\": \"radiology_reports\",\n",
      "    \"shard\": \"1\",\n",
      "    \"prirep\": \"p\",\n",
      "    \"node\": \"es_data\"\n",
      "  },\n",
      "  {\n",
      "    \"index\": \"radiology_reports\",\n",
      "    \"shard\": \"1\",\n",
      "    \"prirep\": \"r\",\n",
      "    \"node\": \"es_master\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Wait for index creation to propagate\n",
    "sleep(0.1)  \n",
    "# Check index shards and replicas location, likely that primary shard is on data node and replica on master node\n",
    "shards_info = es.cat.shards(index=index_name, h=[\"index\", \"shard\", \"prirep\", \"node\"], format=\"json\")\n",
    "print(json.dumps(shards_info.body, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare documents for indexing \n",
    "documents = [] \n",
    "for idx, row in df_clean.iterrows(): \n",
    "    doc = { \n",
    "           \"_index\": index_name, \n",
    "           \"_id\": idx, \n",
    "           \"_source\": { \n",
    "               \"patient_id\": row['Patient ID'], \n",
    "               \"clinicians_notes\": row[\"Clinician's Notes\"] \n",
    "            }\n",
    "    } \n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 515 documents. Failed: 0\n",
      "Total documents in index: 515\n"
     ]
    }
   ],
   "source": [
    "# Index the documents in bulk\n",
    "try:\n",
    "    success, failed = helpers.bulk(es, documents, stats_only=True)\n",
    "    print(f\"Successfully indexed {success} documents. Failed: {failed}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during bulk indexing: {e}\")\n",
    "\n",
    "sleep(1)\n",
    "\n",
    "# Verify the data was indexed\n",
    "count = es.count(index=index_name)['count']\n",
    "print(f\"Total documents in index: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Refresh segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Example of searching for a not-yet-refreshed document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search query can return old results if requested before index refreshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document id 0\n",
      "[\n",
      "  {\n",
      "    \"_index\": \"radiology_reports\",\n",
      "    \"_id\": \"0\",\n",
      "    \"_score\": 1.0,\n",
      "    \"_source\": {\n",
      "      \"patient_id\": 1,\n",
      "      \"clinicians_notes\": \"L4-5: degenerative annular disc bulge is noted more to the left side compressing thecal sac, compressing left nerve root and narrowing right neural foramen. // Evidence of hyperintense signal within the annulus fibrosus at left paramedian/posterolateral area which probably represents a torn annulus.\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "search_query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"patient_id\": 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = es.search(index=index_name, body=search_query)\n",
    "hits = res['hits']['hits']\n",
    "\n",
    "if not hits:\n",
    "    print(\"No documents found for patient_id 1\")\n",
    "else:\n",
    "    hit = hits[0]\n",
    "    doc_id = hit['_id']\n",
    "    try:\n",
    "        es.update(\n",
    "            index=index_name, \n",
    "            id=doc_id, \n",
    "            body={\"doc\": {\"clinicians_notes\": \"unknown\"}}\n",
    "        )\n",
    "        print(f\"Updated document id {doc_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating id {doc_id}: {e}\")\n",
    "\n",
    "    # search will return the old value since refresh has not occurred yet\n",
    "    result = es.search(index=index_name, body=search_query)\n",
    "    print(json.dumps(result['hits']['hits'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but will return updated results after index refreshes => eventual consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"_index\": \"radiology_reports\",\n",
      "    \"_id\": \"0\",\n",
      "    \"_score\": 1.0,\n",
      "    \"_source\": {\n",
      "      \"patient_id\": 1,\n",
      "      \"clinicians_notes\": \"unknown\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "search_query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"patient_id\": 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "result = es.search(index=index_name, body=search_query)\n",
    "print(json.dumps(result['hits']['hits'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ES provides an API to set the refresh interval of an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.put_settings(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"index\": {\n",
    "            \"refresh_interval\": \"2s\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"_index\": \"radiology_reports\",\n",
      "    \"_id\": \"10\",\n",
      "    \"_score\": 1.0,\n",
      "    \"_source\": {\n",
      "      \"patient_id\": 11,\n",
      "      \"clinicians_notes\": \"LSS MRI :\\nAbout 3*2 cm lesion with inhomogenous signal intensity noted just posterior to L1 veretbral body ,largely compressing the Rt side of the thecal sac. contrast study is advsied.\\nDiffuse disc bulge noted at L4/L5 level, compresing the thecal sac and encroaching upon both neural canals.\\nWide base disc bulge noted at L5/S1 level, extending to Lt lateral recess , compressin the thecal sac and nerve roots, more to left side, associated with ligamnetum flavum hyperatrophy. \\n  \"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# query after refresh\n",
    "es.indices.refresh(index=index_name)\n",
    "search_query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"patient_id\": 11\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "result = es.search(index=index_name, body=search_query)\n",
    "print(json.dumps(result['hits']['hits'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full rewrite for document update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_index': 'radiology_reports', '_id': '1001', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 2, 'failed': 0}, '_seq_no': 228, '_primary_term': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial insert\n",
    "es.index(\n",
    "    index=index_name,\n",
    "    id=\"1001\",\n",
    "    body={\n",
    "        \"patient_id\": 1001,\n",
    "        \"clinicians_notes\": \"Initial note\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After initial insert ---\n",
      "  shard segments  num_docs  deleted_docs\n",
      "0     0       _0       287             0\n",
      "1     0       _0       287             0\n",
      "2     1       _0       228             0\n",
      "3     1       _1         1             0\n",
      "4     1       _0       228             0\n",
      "5     1       _1         1             0\n"
     ]
    }
   ],
   "source": [
    "def show_segments(label):\n",
    "    segs = es.indices.segments(index=TEXT_INDEX)\n",
    "    shards = segs[\"indices\"][TEXT_INDEX][\"shards\"]\n",
    "    print(f\"\\n--- {label} ---\")\n",
    "    table_rows = []\n",
    "    for shard_id, shard_list in shards.items():\n",
    "        for shard in shard_list:\n",
    "            for idx, segment in enumerate(shard['segments'].values()):\n",
    "                table_rows.append([\n",
    "                    shard_id,\n",
    "                    list(shard['segments'].keys())[idx],\n",
    "                    segment['num_docs'],\n",
    "                    segment['deleted_docs']\n",
    "                ])\n",
    "    print(pd.DataFrame(table_rows, columns=[\"shard\", \"segments\", \"num_docs\", \"deleted_docs\"]))\n",
    "\n",
    "show_segments(\"After initial insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearch connection closed\n"
     ]
    }
   ],
   "source": [
    "# Close the Elasticsearch connection\n",
    "es.close()\n",
    "print(\"Elasticsearch connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Delete is logical (tombstone) until merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78962/2724439237.py:18: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es.delete(index=TEXT_INDEX, id=18, ignore=[404])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After DELETE (tombstone present) ---\n",
      "  shard segments  num_docs  deleted_docs\n",
      "0     0       _0       287             0\n",
      "1     0       _1         0             1\n",
      "2     0       _0       287             0\n",
      "3     0       _1         0             1\n",
      "4     1       _0       228             0\n",
      "5     1       _1         1             0\n",
      "6     1       _0       228             0\n",
      "7     1       _1         1             0\n",
      "\n",
      "--- After force merge (tombstone cleaned) ---\n",
      "  shard segments  num_docs  deleted_docs\n",
      "0     0       _2       286             1\n",
      "1     0       _2       286             1\n",
      "2     1       _2       229             0\n",
      "3     1       _2       229             0\n"
     ]
    }
   ],
   "source": [
    "def show_segments(label):\n",
    "    segs = es.indices.segments(index=TEXT_INDEX)\n",
    "    shards = segs[\"indices\"][TEXT_INDEX][\"shards\"]\n",
    "    print(f\"\\n--- {label} ---\")\n",
    "    table_rows = []\n",
    "    for shard_id, shard_list in shards.items():\n",
    "        for shard in shard_list:\n",
    "            for idx, segment in enumerate(shard['segments'].values()):\n",
    "                table_rows.append([\n",
    "                    shard_id,\n",
    "                    list(shard['segments'].keys())[idx],\n",
    "                    segment['num_docs'],\n",
    "                    segment['deleted_docs']\n",
    "                ])\n",
    "    print(pd.DataFrame(table_rows, columns=[\"shard\", \"segments\", \"num_docs\", \"deleted_docs\"]))\n",
    "\n",
    "# delete the doc (logical delete => tombstone)\n",
    "es.delete(index=TEXT_INDEX, id=18, ignore=[404])\n",
    "sleep(2)\n",
    "show_segments(\"After DELETE (tombstone present)\")\n",
    "\n",
    "# force merge to 1 segment (physical removal)\n",
    "es.indices.forcemerge(index=TEXT_INDEX, max_num_segments=1)\n",
    "sleep(2)\n",
    "show_segments(\"After force merge (tombstone cleaned)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After force merge (tombstone cleaned) ---\n",
      "  shard segments  num_docs  deleted_docs\n",
      "0     0       _3       286             0\n",
      "1     0       _3       286             0\n",
      "2     1       _2       229             0\n",
      "3     1       _2       229             0\n"
     ]
    }
   ],
   "source": [
    "es.indices.forcemerge(index=TEXT_INDEX, max_num_segments=1)\n",
    "show_segments(\"After force merge (tombstone cleaned)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbms-assignment (3.13.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
